{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "616703cc-45ac-4302-8ef5-dee6776ad5d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import base64\n",
    "import tempfile\n",
    "import os\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5c25f47-65ba-4efb-8224-b58552e919e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 1. CATALOG & CONFIG HELPERS ---\n",
    "\n",
    "def load_catalog(catalog_path):\n",
    "    return json.loads(dbutils.fs.head(catalog_path, 1_000_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00dd13c5-eecb-4e5d-8440-7fa897a7f58c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG_TO_SPARK_TYPE = {\n",
    "    \"string\": \"string\",\n",
    "    \"integer\": \"long\",      # safer for MySQL / BigQuery\n",
    "    \"number\": \"double\",\n",
    "    \"boolean\": \"boolean\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89e90426-1396-4fcd-b6e3-8774ca6d77a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def build_table_mappings(catalog):\n",
    "    tables = []\n",
    "\n",
    "    for s in catalog.get(\"catalog\", {}).get(\"streams\", []):\n",
    "        selected_cols = []\n",
    "        column_types = {}\n",
    "\n",
    "        properties = s.get(\"schema\", {}).get(\"properties\", {})\n",
    "\n",
    "        for m in s.get(\"metadata\", []):\n",
    "            breadcrumb = m.get(\"breadcrumb\")\n",
    "            meta = m.get(\"metadata\", {})\n",
    "\n",
    "            if (\n",
    "                meta.get(\"selected\")\n",
    "                and isinstance(breadcrumb, list)\n",
    "                and len(breadcrumb) >= 2\n",
    "                and breadcrumb[0] == \"properties\"\n",
    "            ):\n",
    "                col_name = breadcrumb[-1]\n",
    "                selected_cols.append(col_name)\n",
    "\n",
    "                col_schema = properties.get(col_name, {})\n",
    "                types = col_schema.get(\"type\", [])\n",
    "                fmt = col_schema.get(\"format\")\n",
    "\n",
    "                if fmt == \"date-time\":\n",
    "                    column_types[col_name] = \"timestamp\"\n",
    "                else:\n",
    "                    base_type = next(t for t in types if t != \"null\")\n",
    "                    column_types[col_name] = CATALOG_TO_SPARK_TYPE.get(base_type, \"string\")\n",
    "\n",
    "        tables.append({\n",
    "            \"source_table\": s[\"stream\"],\n",
    "            \"destination_table\": s[\"destination_table\"],\n",
    "            \"replication_method\": s[\"replication_method\"],\n",
    "            \"bookmark_columns\": s.get(\"bookmark_properties\", []),\n",
    "            \"unique_constraints\": s.get(\"unique_constraints\", []),\n",
    "            \"unique_conflict_method\": s.get(\"unique_conflict_method\"),\n",
    "            \"selected_columns\": selected_cols,\n",
    "            \"column_types\": column_types   \n",
    "        })\n",
    "\n",
    "    return tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a88fc32-3a21-4f9e-b5b5-d5c6680273dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "def cast_df_from_catalog(df, column_types):\n",
    "    for col_name, spark_type in column_types.items():\n",
    "        if col_name in df.columns:\n",
    "            df = df.withColumn(col_name, col(col_name).cast(spark_type))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "866bc5d1-5a76-402c-9d35-341c38e3a4e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_bq_client(bq_cfg):\n",
    "    key_json = base64.b64decode(bq_cfg[\"credentials_b64\"]).decode()\n",
    "    creds_info = json.loads(key_json)\n",
    "    credentials = service_account.Credentials.from_service_account_info(creds_info)\n",
    "    return bigquery.Client(credentials=credentials, project=bq_cfg[\"project_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "220e7c50-575c-4ef7-9cfa-8a207c51a6e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 2. SCHEMA EVOLUTION HELPERS ---\n",
    "\n",
    "def pandas_dtype_to_bq(dtype):\n",
    "    dtype = str(dtype)\n",
    "    if dtype == \"Int64\":\n",
    "        return \"INT64\"\n",
    "    elif dtype == \"object\":\n",
    "        return \"STRING\"\n",
    "    elif dtype.startswith(\"datetime64\"):\n",
    "        return \"DATETIME\"\n",
    "    elif dtype=='float64':\n",
    "        return \"FLOAT64\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dtype: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b7e6d1f-ab9e-4339-862a-007a5edef35a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def spark_type_to_bq(f_type):\n",
    "    \"\"\"Maps Spark DataType to BQ SQL types, keeping DATETIME for timestamps.\"\"\"\n",
    "     \n",
    "    if isinstance(f_type, TimestampType):\n",
    "        return \"DATETIME\"  # Maintains the datetime64 requirement\n",
    "    elif isinstance(f_type, (DecimalType, LongType, IntegerType)):\n",
    "        return \"INT64\"\n",
    "    elif isinstance(f_type, BooleanType):\n",
    "        return \"BOOL\"\n",
    "    elif isinstance(f_type, (FloatType, DoubleType)):\n",
    "        return \"FLOAT64\"\n",
    "    else:\n",
    "        return \"STRING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acccfff7-4b03-4765-bd30-004136783818",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def evolve_bq_schema(\n",
    "    bq_cfg,\n",
    "    target_table,\n",
    "    df_schema,\n",
    "    client,\n",
    "    replication_method,\n",
    "    staging_table_exists=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Detect new columns from Spark DF and add them to BigQuery target\n",
    "    (and staging table if incremental).\n",
    "    \"\"\"\n",
    "\n",
    "    target_full_path = f\"{bq_cfg['project_id']}.{bq_cfg['dataset']}.{target_table}\"\n",
    "    staging_full_path = f\"{bq_cfg['project_id']}.{bq_cfg['dataset']}.{target_table}_staging\"\n",
    "\n",
    "    # Fetch target schema\n",
    "    bq_table = client.get_table(target_full_path)\n",
    "    existing_columns = {\n",
    "        field.name.lower()\n",
    "        for field in bq_table.schema\n",
    "        if not field.name.lower().startswith(\"_mage\")\n",
    "    }\n",
    "\n",
    "    # Detect new columns\n",
    "    new_columns_ddl = []\n",
    "    for field in df_schema:\n",
    "        if field.name.lower() not in existing_columns:\n",
    "            bq_type = spark_type_to_bq(field.dataType)\n",
    "            new_columns_ddl.append(f\"ADD COLUMN `{field.name}` {bq_type}\")\n",
    "\n",
    "    if not new_columns_ddl:\n",
    "        print(\"No new columns from source\")\n",
    "        return\n",
    "\n",
    "    # Alter target table\n",
    "    alter_target_sql = f\"\"\"\n",
    "        ALTER TABLE `{target_full_path}`\n",
    "        {', '.join(new_columns_ddl)}\n",
    "    \"\"\"\n",
    "    print(f\"Schema Evolution for target_table {target_table}\")\n",
    "    client.query(alter_target_sql).result()\n",
    "\n",
    "    # Alter staging table only for incremental loads\n",
    "    if replication_method == \"INCREMENTAL\" and staging_table_exists:\n",
    "        alter_staging_sql = f\"\"\"\n",
    "            ALTER TABLE `{staging_full_path}`\n",
    "            {', '.join(new_columns_ddl)}\n",
    "        \"\"\"\n",
    "        print(f\"Schema Evolution for {staging_table}\")\n",
    "        client.query(alter_staging_sql).result()\n",
    "    else:\n",
    "        print(\"Staging table schema evolution skipped (FULL load)\")\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "894918e3-778d-46cd-b79d-e8ca2b612af8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 3. DATA TRANSFORMATIONS ---\n",
    "\n",
    "def normalize_decimals(df):\n",
    "    for f in df.schema.fields:\n",
    "        if isinstance(f.dataType, DecimalType):\n",
    "            df = df.withColumn(f.name, F.col(f.name).cast(\"long\"))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d0b1073-07fa-4c75-883f-ff106937f6af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def boolean_to_int64(df):\n",
    "    for f in df.schema.fields:\n",
    "        if isinstance(f.dataType, BooleanType):\n",
    "            df = df.withColumn(f.name, F.col(f.name).cast('byte'))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8c324f5-2030-4b0a-a825-05150119e451",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def normalize_timestamps(df):\n",
    "    \"\"\"\n",
    "    Ensures the function returns TWO values: the dataframe and the list of modified columns.\n",
    "    \"\"\"\n",
    "    for f in df.schema.fields:\n",
    "        if isinstance(f.dataType, (TimestampType, DateType)):\n",
    "            df = df.withColumn(\n",
    "                f.name,\n",
    "                F.date_format(F.col(f.name), \"yyyy-MM-dd HH:mm:ss\")\n",
    "            )            \n",
    "    # This must return a tuple of (DataFrame, List)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52b36155-e596-4b56-adca-8e4a7841c88d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# DATABASE CONFIGS\n",
    "def get_mysql_config(secret_scope):\n",
    "    db_name = dbutils.secrets.get(secret_scope, \"database\")\n",
    "    host = dbutils.secrets.get(secret_scope, \"host\")\n",
    "    port = dbutils.secrets.get(secret_scope, \"port\")\n",
    "    user = dbutils.secrets.get(secret_scope, \"username\")\n",
    "    password = dbutils.secrets.get(secret_scope, \"password\")\n",
    "    jdbc_url = f\"jdbc:mysql://{host}:{port}/{db_name}?useSSL=false&serverTimezone=UTC\"\n",
    "    return {\"jdbc_url\": jdbc_url, \"user\": user, \"password\": password}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd68cd0e-0864-49aa-bd73-f338d1fd2022",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_bigquery_config(secret_scope, secret_prefix_bq, bq_sa_name):\n",
    "    project_id = dbutils.secrets.get(secret_scope, \"project_id\")\n",
    "    dataset = dbutils.secrets.get(secret_scope, f\"{secret_prefix_bq}_dataset\")\n",
    "    staging_dataset = dbutils.secrets.get(secret_scope, \"staging_dataset\")\n",
    "    key_json = dbutils.secrets.get(secret_scope, bq_sa_name)\n",
    "    key_b64 = base64.b64encode(key_json.encode()).decode()\n",
    "    return {\"project_id\": project_id, \"dataset\": dataset, \"credentials_b64\": key_b64, \"staging_dataset\": staging_dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4639978b-38b9-4dfb-b09f-20ee5b91d600",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def table_exists_check(client, bq_cfg, staging_table):\n",
    "    try:\n",
    "        client.get_table(f\"{bq_cfg['project_id']}.{bq_cfg['staging_dataset']}.{staging_table}\")\n",
    "        staging_table_exists = True\n",
    "        return staging_table_exists\n",
    "    except:\n",
    "        staging_table_exists = False\n",
    "        return staging_table_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd7bf25e-3b6c-4335-af1b-1bb628fab135",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_number_type_columns_for_all_streams(catalog):\n",
    "    \"\"\"\n",
    "    Returns a dict:\n",
    "    {\n",
    "        stream_name: [number_type_columns]\n",
    "    }\n",
    "    \"\"\"\n",
    "    stream_number_cols = {}\n",
    "\n",
    "    streams = catalog[\"catalog\"][\"streams\"]\n",
    "\n",
    "    for stream in streams:\n",
    "        stream_name = stream[\"stream\"]\n",
    "        schema_props = stream[\"schema\"][\"properties\"]\n",
    "\n",
    "        number_cols = [\n",
    "            col_name\n",
    "            for col_name, col_def in schema_props.items()\n",
    "            if \"number\" in col_def.get(\"type\", [])\n",
    "        ]\n",
    "\n",
    "        if number_cols:\n",
    "            stream_number_cols[stream_name] = number_cols\n",
    "\n",
    "    return stream_number_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ca338c8-522a-4ded-9db2-3587a04be2e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def cast_number_columns(df, number_columns):\n",
    "    for col in number_columns:\n",
    "        if col in df.columns:\n",
    "            df = df.withColumn(col, F.col(col).cast(DoubleType()))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ecb9bc95-c448-427f-a24b-65ef81e9c66b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 4. MAIN PIPELINE ---\n",
    "\n",
    "def run_pipeline(\n",
    "    catalog_path,\n",
    "    mysql_secret_scope,\n",
    "    bq_secret_scope,\n",
    "    bq_sa_name,\n",
    "    secret_prefix_bq,\n",
    "    target_table_prefix,\n",
    "    start_date,\n",
    "    end_date\n",
    "):\n",
    "    # Initialization\n",
    "    catalog = load_catalog(catalog_path)\n",
    "    tables = build_table_mappings(catalog)\n",
    "    mysql_cfg = get_mysql_config(mysql_secret_scope)\n",
    "    bq_cfg = get_bigquery_config(bq_secret_scope, secret_prefix_bq, bq_sa_name)\n",
    "    key_b64 = bq_cfg[\"credentials_b64\"]\n",
    "    client = get_bq_client(bq_cfg)\n",
    "    print(f'********************************************************************',bq_cfg[\"staging_dataset\"])\n",
    "    # Get number columns for all tables\n",
    "    number_columns_map = get_number_type_columns_for_all_streams(catalog)\n",
    "    print(f\"Number columns map:{number_columns_map}\")\n",
    "\n",
    "    for t in tables:\n",
    "        src_table = t[\"source_table\"]\n",
    "        dest_table = t[\"destination_table\"]\n",
    "        target_table = f\"{target_table_prefix}_{dest_table}\"\n",
    "        target_full_path = f\"{bq_cfg['project_id']}.{bq_cfg['dataset']}.{target_table}\"\n",
    "\n",
    "        print(f\"\\nProcessing table: {dest_table}\")\n",
    "\n",
    "        # Check existence\n",
    "        table_exists = False\n",
    "        try:\n",
    "            client.get_table(target_full_path)\n",
    "            table_exists = True\n",
    "        except Exception:\n",
    "            table_exists = False\n",
    "\n",
    "        # Incremental logic: Bookmark fetch\n",
    "        bookmark_col = t[\"bookmark_columns\"][0] if t[\"bookmark_columns\"] else None\n",
    "        where_clause = \"1=1\"\n",
    "\n",
    "        # --------------------------------------------------\n",
    "        # Step 1: Build WHERE clause\n",
    "        # --------------------------------------------------\n",
    "\n",
    "        if start_date and end_date:\n",
    "            # Date-range based UPSERT\n",
    "            where_clause = (\n",
    "                f\"{bookmark_col} >= '{start_date}' \"\n",
    "                f\"AND {bookmark_col} < '{end_date}'\"\n",
    "            )\n",
    "            print(f\"Date range UPSERT -> {start_date} to {end_date}\")\n",
    "\n",
    "        elif t[\"replication_method\"] == \"INCREMENTAL\" and table_exists and bookmark_col:\n",
    "            try:\n",
    "                max_query = f\"SELECT MAX({bookmark_col}) FROM `{target_full_path}`\"\n",
    "                res = list(client.query(max_query).result())\n",
    "                last_val = res[0][0]\n",
    "                if last_val:\n",
    "                    where_clause = f\"{bookmark_col} > '{last_val}'\"\n",
    "                    print(f\"Incremental Filter: {where_clause}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping bookmark fetch: {e}\")\n",
    "\n",
    "        # Read Source Data\n",
    "        query = f\"(SELECT * FROM {src_table} WHERE {where_clause}) t\"\n",
    "        df = (\n",
    "            spark.read.format(\"jdbc\")\n",
    "            .option(\"url\", mysql_cfg[\"jdbc_url\"])\n",
    "            .option(\"dbtable\", query)\n",
    "            .option(\"user\", mysql_cfg[\"user\"])\n",
    "            .option(\"password\", mysql_cfg[\"password\"])\n",
    "            .load()\n",
    "        )\n",
    "\n",
    "        if df.isEmpty():\n",
    "            print(\"No data to process.\")\n",
    "            continue\n",
    "        print(f\"Rows fetched -> {df.count()}\")\n",
    "\n",
    "        if t[\"selected_columns\"]:\n",
    "            df = df.select(*[c for c in t[\"selected_columns\"] if c in df.columns])\n",
    "\n",
    "        df = cast_df_from_catalog(df, t[\"column_types\"])\n",
    "        \n",
    "        if src_table in number_columns_map:\n",
    "            df = cast_number_columns(df, number_columns_map[src_table])\n",
    "\n",
    "        df = normalize_decimals(df)\n",
    "        df = boolean_to_int64(df)\n",
    "\n",
    "        # Write Logic\n",
    "        if t[\"replication_method\"] == \"FULL_TABLE\":\n",
    "            evolve_bq_schema(bq_cfg, target_table, df.schema, client, t[\"replication_method\"])\n",
    "            df = normalize_timestamps(df)\n",
    "            df.write.format(\"bigquery\")\\\n",
    "            .option(\"credentials\", key_b64)\\\n",
    "            .option(\"parentProject\", bq_cfg[\"project_id\"])\\\n",
    "            .option(\"project\", bq_cfg[\"project_id\"])\\\n",
    "            .option(\"dataset\", bq_cfg[\"dataset\"])\\\n",
    "            .option(\"table\", target_table)\\\n",
    "            .option(\"allowFieldAddition\", \"true\")\\\n",
    "            .option('writeMethod', 'direct')\\\n",
    "            .mode(\"overwrite\")\\\n",
    "            .save()\n",
    "            print(f\"Full load completed {target_table}.\")\n",
    "\n",
    "        else:\n",
    "            # Method 3: Merge Schema + Dynamic Merge\n",
    "            staging_table = f\"{target_table}_staging\"\n",
    "            staging_table_exists = table_exists_check(client, bq_cfg, staging_table)\n",
    "\n",
    "            evolve_bq_schema(bq_cfg, target_table, df.schema, client, t[\"replication_method\"], staging_table_exists)\n",
    "            df = normalize_timestamps(df)\n",
    "            \n",
    "            query = f\"\"\"\n",
    "                    SELECT *\n",
    "                    FROM `{bq_cfg['project_id']}.{bq_cfg['dataset']}.{target_table}`\n",
    "                    \"\"\"\n",
    "            target_df = client.query(query).to_dataframe()\n",
    "            target_columns = target_df.columns.tolist()\n",
    "\n",
    "            if not staging_table_exists:\n",
    "                columns_ddl = []\n",
    "\n",
    "                for col, dtype in target_df.dtypes.items():\n",
    "                    bq_type = pandas_dtype_to_bq(dtype)\n",
    "                    columns_ddl.append(f\"`{col}` {bq_type}\")\n",
    "\n",
    "                columns_sql = \",\\n  \".join(columns_ddl)\n",
    "\n",
    "                create_table_sql = f\"\"\"\n",
    "                CREATE OR REPLACE TABLE `{bq_cfg['project_id']}.{bq_cfg['staging_dataset']}.{staging_table}` (\n",
    "                {columns_sql}\n",
    "                )\n",
    "                \"\"\"\n",
    "                client.query(create_table_sql).result()\n",
    "                print(\"Staging Table created successfully\")\n",
    "            else:\n",
    "                print(\"Staging Table already exists\")\n",
    "            \n",
    "            display(df)\n",
    "            # Write batch to staging\n",
    "            df.write.format(\"bigquery\")\\\n",
    "            .option(\"credentials\", key_b64)\\\n",
    "            .option(\"parentProject\", bq_cfg[\"project_id\"])\\\n",
    "            .option(\"project\", bq_cfg[\"project_id\"])\\\n",
    "            .option(\"dataset\", bq_cfg[\"staging_dataset\"])\\\n",
    "            .option(\"table\", staging_table)\\\n",
    "            .option(\"allowFieldAddition\", \"true\")\\\n",
    "            .option(\"writeMethod\", \"direct\")\\\n",
    "            .mode(\"overwrite\")\\\n",
    "            .save()\n",
    "\n",
    "            if t[\"unique_conflict_method\"] == \"UPDATE\" and t[\"unique_constraints\"]:\n",
    "                merge_condition = \" AND \".join(\n",
    "                    [f\"T.`{k}` = S.`{k}`\" for k in t[\"unique_constraints\"]]\n",
    "                )\n",
    "                update_columns = \",\\n\".join([f\"T.{c} = S.{c}\" for c in target_columns])\n",
    "                \n",
    "                merge_sql = f\"\"\"\n",
    "                MERGE `{bq_cfg['project_id']}.{bq_cfg['dataset']}.{target_table}` T\n",
    "                USING `{bq_cfg['project_id']}.{bq_cfg['staging_dataset']}.{staging_table}` S\n",
    "                ON {merge_condition}\n",
    "                WHEN MATCHED THEN\n",
    "                UPDATE SET {update_columns}\n",
    "                WHEN NOT MATCHED THEN\n",
    "                INSERT ROW\n",
    "                \"\"\"\n",
    "                client.query(merge_sql).result()\n",
    "                print(\"*****************Merge Completed*****************\")\n",
    "\n",
    "            client.query(\n",
    "                f\"TRUNCATE TABLE `{bq_cfg['project_id']}.{bq_cfg['staging_dataset']}.{staging_table}`\"\n",
    "            ).result()\n",
    "            print(\"\\nPipeline execution finished\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "NB_common_utils",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
